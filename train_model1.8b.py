{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd5a14-29ac-4d88-985a-cc6ad3e59d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCELoss, BCEWithLogitsLoss, MSELoss\n",
    "from model.dcn import My_DeepCrossNetworkModel_withCommentsRanking # (确保 model/dcn.py 是我们修正过的“快速版”)\n",
    "from utils.set_seed import setup_seed\n",
    "from utils.summary_dat import cal_comments_dims, make_feature_with_comments, cal_field_dims\n",
    "from utils.data_wrapper import Wrap_Dataset, Wrap_Dataset4\n",
    "from utils.early_stop import EarlyStopping2\n",
    "from utils.loss import ListMLELoss\n",
    "from utils.evaluate import cal_group_metric, cal_reg_metric\n",
    "from preprocessing.cal_ground_truth import cal_ground_truth\n",
    "\n",
    "class Learner(object):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        self.dat_name = args.dat_name\n",
    "        self.model_name = args.model_name\n",
    "        self.label_name = args.label_name\n",
    "\n",
    "        self.group_num = args.group_num\n",
    "        self.windows_size = args.windows_size\n",
    "        self.eps = args.eps\n",
    "\n",
    "        self.batch_size = args.batch_size\n",
    "        self.lr = args.lr\n",
    "        self.weight_decay = args.weight_decay\n",
    "        self.patience = args.patience\n",
    "        self.use_cuda = args.use_cuda\n",
    "        self.epoch_num = args.epoch_num\n",
    "        self.seed = args.randseed\n",
    "        self.fout = args.fout\n",
    "\n",
    "        self.noise_point = args.noise_point\n",
    "        self.bias_point = args.bias_point\n",
    "        if args.dat_name == 'KuaiComt':\n",
    "            if args.label_name == 'WLR':\n",
    "                self.label_name = 'long_view2'\n",
    "                self.weight_name = 'weighted_st'\n",
    "                self.label2_name = 'comments_score'\n",
    "                self.label1_name = 'user_clicked'\n",
    "\n",
    "        self.load_to_eval = args.load_to_eval\n",
    "        # (关键!) 设置你找到的最佳参数\n",
    "        self.lambda1 = 0.001 \n",
    "        self.lambda2 = 0.1\n",
    "        print(f\"Using Hyperparameters: lambda1={self.lambda1}, lambda2={self.lambda2}\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # (关键!) 嵌入表加载开关\n",
    "        # -----------------------------\n",
    "        # 切换这个变量来运行两个实验\n",
    "        # '1.8B' = 运行 1.8B 实验组\n",
    "        # '7B'   = 运行 7B 新基线\n",
    "        self.EMBEDDING_MODE = '1.8B' # <-- 在这里切换\n",
    "        \n",
    "        print(f\"--- RUNNING IN {self.EMBEDDING_MODE} MODE ---\")\n",
    "        \n",
    "        if self.EMBEDDING_MODE == '1.8B':\n",
    "            VIDEO_EMB_PATH = '../rec_datasets/KuaiComt/video_embeddings_qwen1.8b_tiny.pt'\n",
    "            COMMENT_EMB_PATH = '../rec_datasets/KuaiComt/comment_embeddings_qwen1.8b_tiny.pt'\n",
    "        else: # '7B'\n",
    "            # (注意!) 更改为你的 7B 原始嵌入表路径\n",
    "            VIDEO_EMB_PATH = '../rec_datasets/KuaiComt/video_embeddings_qwen7b_tiny.pt'\n",
    "            COMMENT_EMB_PATH = '../rec_datasets/KuaiComt/comment_embeddings_qwen7b_tiny.pt'\n",
    "\n",
    "        print(f\"Loading video embeddings (cpu) from: {VIDEO_EMB_PATH}\")\n",
    "        # (修正!) 你的 1.8B 嵌入保存为字典, 7B 保存为完整张量, 我们需要统一处理\n",
    "        video_embeddings_data = torch.load(VIDEO_EMB_PATH)\n",
    "        if isinstance(video_embeddings_data, dict):\n",
    "            # 这是你的 1.8B 字典 (短 ID -> 嵌入)\n",
    "            # (注意!) 我们假设 7B 也是字典 (短 ID -> 嵌入)。如果 7B 是张量, 这里的逻辑需要修改\n",
    "            video_ids_sorted = sorted(video_embeddings_data.keys())\n",
    "            video_embeddings_list = [video_embeddings_data[k] for k in video_ids_sorted]\n",
    "            video_embeddings_tensor = torch.stack(video_embeddings_list).to(dtype=torch.float32).cpu()\n",
    "            # (关键!) 你的 ID 已经是整数了\n",
    "            self.video_id2idx = {vid: i for i, vid in enumerate(video_ids_sorted)}\n",
    "        else:\n",
    "            # 这是 7B 完整张量 (假设它很大)\n",
    "            video_embeddings_tensor = video_embeddings_data.to(dtype=torch.float32).cpu()\n",
    "            # (注意!) 这里的 ID 映射逻辑依赖 7B 嵌入表的原始格式\n",
    "            # 我们假设 7B 嵌入的索引与 \"短 ID\" (行号) 一致\n",
    "            self.video_id2idx = {i: i for i in range(len(video_embeddings_tensor))} \n",
    "            print(\"Warning: Assuming 7B video embedding index matches short ID.\")\n",
    "\n",
    "        video_embeddings_tensor.requires_grad = False\n",
    "        self.photo_embeddings = video_embeddings_tensor\n",
    "        print(f\"Loaded video embeddings: {self.photo_embeddings.shape}\")\n",
    "\n",
    "\n",
    "        print(f\"Loading comment embeddings (cpu) from: {COMMENT_EMB_PATH}\")\n",
    "        comment_embeddings_data = torch.load(COMMENT_EMB_PATH)\n",
    "        if isinstance(comment_embeddings_data, dict):\n",
    "            # 这是你的 1.8B 字典 (短 ID -> 嵌入)\n",
    "            comment_ids_sorted = sorted(comment_embeddings_data.keys())\n",
    "            comment_embeddings_list = [comment_embeddings_data[k] for k in comment_ids_sorted]\n",
    "            comment_embeddings_tensor = torch.stack(comment_embeddings_list).to(dtype=torch.float32).cpu()\n",
    "            self.comment_id2idx = {cid: i for i, cid in enumerate(comment_ids_sorted)}\n",
    "        else:\n",
    "            # 这是 7B 完整张量\n",
    "            comment_embeddings_tensor = comment_embeddings_data.to(dtype=torch.float32).cpu()\n",
    "            self.comment_id2idx = {i: i for i in range(len(comment_embeddings_tensor))}\n",
    "            print(\"Warning: Assuming 7B comment embedding index matches short ID.\")\n",
    "\n",
    "        comment_embeddings_tensor.requires_grad = False\n",
    "        self.comment_embeddings = comment_embeddings_tensor\n",
    "        print(f\"Loaded comment embeddings: {self.comment_embeddings.shape}\")\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        setup_seed(self.seed)\n",
    "        self.all_dat, self.train_dat, self.vali_dat, self.test_dat = self._load_and_spilt_dat()\n",
    "        self.train_loader, self.vali_loader, self.test_loader = self._wrap_dat()\n",
    "        self.model,self.c_model, self.optim, self.c_optim, self.early_stopping = self._init_train_env()\n",
    "        if not self.load_to_eval:\n",
    "            self._train_iteration()\n",
    "        self._test_and_save()\n",
    "\n",
    "    @staticmethod\n",
    "    def _cal_correct_wt(row, sigma=1.0):\n",
    "        d = row['duration_ms']\n",
    "        wt = row['play_time_truncate']\n",
    "        return wt - 1 * (norm.pdf((d - wt)/sigma) / norm.cdf((d - wt)/sigma))\n",
    "\n",
    "    def _load_and_spilt_dat(self):\n",
    "        if self.dat_name == 'KuaiComt':\n",
    "            # (关键!) 加载你采样的 2% TINY 数据集\n",
    "            print(\"--- LOADING TINY 2% DATASET ---\")\n",
    "            all_dat = pd.read_csv('../rec_datasets/WM_KuaiComt/KuaiComt_TINY_subset.csv', sep=',')\n",
    "            \n",
    "            # (注意!) 你需要确保 cal_ground_truth 仍然有效\n",
    "            all_dat = cal_ground_truth(all_dat, self.dat_name)\n",
    "            \n",
    "            # (注意!) 这里的日期划分在 TINY 数据集上可能导致验证/测试集为空\n",
    "            # 我们将使用 80/10/10 的随机划分代替\n",
    "            \n",
    "            # all_dat = all_dat.sample(frac=1, random_state=self.seed).reset_index(drop=True)\n",
    "            # n = len(all_dat)\n",
    "            # n_train = int(n * 0.8)\n",
    "            # n_vali = int(n * 0.1)\n",
    "            # train_dat = all_dat.iloc[:n_train]\n",
    "            # vali_dat = all_dat.iloc[n_train : n_train + n_vali]\n",
    "            # test_dat = all_dat.iloc[n_train + n_vali :]\n",
    "\n",
    "            # (更新!) 保持和基线一致的日期划分, 确保TINY数据集中仍然有数据\n",
    "            print(\"Using original date splits on TINY dataset...\")\n",
    "            train_dat = all_dat[(all_dat['date'] <= 2023102199) & (all_dat['date'] >= 2023100100)]\n",
    "            vali_dat = all_dat[(all_dat['date'] <= 2023102699) & (all_dat['date'] >= 2023102200)]\n",
    "            test_dat = all_dat[(all_dat['date'] <= 2023103199) & (all_dat['date'] >= 2023102700)]\n",
    "            \n",
    "            print(f\"Train samples: {len(train_dat)}, Vali samples: {len(vali_dat)}, Test samples: {len(test_dat)}\")\n",
    "            if len(vali_dat) == 0 or len(test_dat) == 0:\n",
    "                print(\"--- WARNING! Validation or Test set is empty! Date splitting failed on tiny data. ---\")\n",
    "                print(\"--- Reverting to 80/10/10 random split. ---\")\n",
    "                all_dat = all_dat.sample(frac=1, random_state=self.seed).reset_index(drop=True)\n",
    "                n = len(all_dat)\n",
    "                n_train = int(n * 0.8)\n",
    "                n_vali = int(n * 0.1)\n",
    "                train_dat = all_dat.iloc[:n_train]\n",
    "                vali_dat = all_dat.iloc[n_train : n_train + n_vali]\n",
    "                test_dat = all_dat.iloc[n_train + n_vali :]\n",
    "                print(f\"Random Split -> Train: {len(train_dat)}, Vali: {len(vali_dat)}, Test: {len(test_dat)}\")\n",
    "\n",
    "        return all_dat, train_dat, vali_dat, test_dat\n",
    "\n",
    "    # _wrap_dat (保持不变)\n",
    "    def _wrap_dat(self):\n",
    "        print(\"Wrapping data...\")\n",
    "        input_train = Wrap_Dataset4(make_feature_with_comments(self.train_dat, self.dat_name),\n",
    "                                    self.train_dat[self.label_name].tolist(),\n",
    "                                    self.train_dat[self.weight_name].tolist(),\n",
    "                                    self.train_dat[self.label1_name].tolist(),\n",
    "                                    self.train_dat[self.label2_name].tolist(), False)\n",
    "        train_loader = DataLoader(input_train, \n",
    "                                  batch_size=self.batch_size, \n",
    "                                  shuffle=True)\n",
    "\n",
    "        input_vali = Wrap_Dataset(make_feature_with_comments(self.vali_dat, self.dat_name),\n",
    "                                  self.vali_dat[self.label_name].tolist(),\n",
    "                                  self.vali_dat[self.weight_name].tolist())\n",
    "        vali_loader = DataLoader(input_vali, \n",
    "                                 batch_size=2048, \n",
    "                                 shuffle=False)\n",
    "\n",
    "        input_test = Wrap_Dataset(make_feature_with_comments(self.test_dat, self.dat_name),\n",
    "                                  self.test_dat[self.label_name].tolist(),\n",
    "                                  self.test_dat[self.weight_name].tolist())\n",
    "        test_loader = DataLoader(input_test, \n",
    "                                 batch_size=2048, \n",
    "                                 shuffle=False)\n",
    "        return train_loader, vali_loader, test_loader\n",
    "\n",
    "    # _init_train_env (使用 \"快速\" dcn.py 的逻辑)\n",
    "    def _init_train_env(self):\n",
    "        print(\"Initializing model...\")\n",
    "        if self.model_name == 'DCN':\n",
    "            # (关键!) 将 CPU 嵌入表 和 id->idx 映射表 打包传递\n",
    "            text_embeddings_bundle = {\n",
    "                \"video_emb_tensor_cpu\": self.photo_embeddings,\n",
    "                \"video_id2idx\": self.video_id2idx,\n",
    "                \"comment_emb_tensor_cpu\": self.comment_embeddings,\n",
    "                \"comment_id2idx\": self.comment_id2idx\n",
    "            }\n",
    "\n",
    "            model = My_DeepCrossNetworkModel_withCommentsRanking(field_dims=cal_field_dims(self.all_dat, self.dat_name),\n",
    "                                                                comments_dims=cal_comments_dims(self.all_dat, self.dat_name),\n",
    "                                                                embed_dim=10, num_layers=3, mlp_dims=[64,64,64], dropout=0.2,\n",
    "                                                                text_embeddings=text_embeddings_bundle) # <-- 传递 bundle\n",
    "            c_model = My_DeepCrossNetworkModel_withCommentsRanking(field_dims=cal_field_dims(self.all_dat, self.dat_name),\n",
    "                                                                comments_dims=cal_comments_dims(self.all_dat, self.dat_name),\n",
    "                                                                embed_dim=10, num_layers=3, mlp_dims=[64,64,64], dropout=0.2,\n",
    "                                                                text_embeddings=text_embeddings_bundle) # <-- 传递 bundle\n",
    "\n",
    "        if self.use_cuda:\n",
    "            model = model.cuda()\n",
    "            c_model = c_model.cuda()\n",
    "\n",
    "        lr = 1e-4\n",
    "        optim = Adam(model.parameters(), lr=lr, weight_decay=self.weight_decay)\n",
    "        c_optim = Adam(c_model.parameters(), lr=lr, weight_decay=self.weight_decay)\n",
    "\n",
    "        early_stopping = EarlyStopping2(self.fout + '_temp', patience=self.patience, verbose=True)\n",
    "\n",
    "        print(model)\n",
    "        return model, c_model, optim, c_optim, early_stopping \n",
    "\n",
    "    # _train_iteration (保持不变)\n",
    "    def _train_iteration(self):\n",
    "        dur=[]\n",
    "        for epoch in range(self.epoch_num):\n",
    "            if epoch >= 0:\n",
    "                t0 = time.time()\n",
    "            loss_log = []\n",
    "            c_loss_log = []\n",
    "\n",
    "            self.model.train()\n",
    "            self.c_model.train()\n",
    "\n",
    "            for _id, batch in enumerate(self.train_loader):\n",
    "                batch = [item.cuda() for item in batch]\n",
    "                self.c_model.train()\n",
    "                self.c_optim.zero_grad()\n",
    "                BCELossfunc = BCEWithLogitsLoss()\n",
    "                output_score = self.c_model(batch[0])\n",
    "                output_score = output_score.view(batch[0].size(0))\n",
    "                target = batch[1]\n",
    "                train_loss = BCELossfunc(output_score, target)\n",
    "                train_loss.backward()\n",
    "                self.c_optim.step()\n",
    "                c_loss_log.append(train_loss.item())\n",
    "\n",
    "                self.model.train()\n",
    "                self.optim.zero_grad()\n",
    "                BCELossfunc = BCEWithLogitsLoss(weight=batch[2])\n",
    "                BCELossfunc2 = BCELoss()\n",
    "                ListMLEfunc = ListMLELoss()\n",
    "                output_score = self.model(batch[0])\n",
    "                comments_score = self.model.get_comment_probs()\n",
    "                comments_score_ = self.model.get_comment_probs_()\n",
    "                output_score = output_score.view(batch[0].size(0))\n",
    "                comments_score = comments_score.view(batch[0].size(0), -1)\n",
    "                comments_score_ = comments_score_.view(batch[0].size(0), -1)\n",
    "                target = batch[1]\n",
    "                train_loss = BCELossfunc(output_score, target)        \n",
    "                label_sums = batch[3].sum(dim=1)\n",
    "                mask = label_sums > 0\n",
    "                masked_output = comments_score[mask]\n",
    "                masked_target = batch[3][mask]\n",
    "                if masked_output.numel() > 0: \n",
    "                    train_loss += self.lambda1 * BCELossfunc2(masked_output, masked_target)\n",
    "                train_loss += self.lambda2 * ListMLEfunc(comments_score_, batch[4])\n",
    "                train_loss.backward()\n",
    "                self.optim.step()\n",
    "                loss_log.append(train_loss.item())\n",
    "\n",
    "            if self.weight_name == 'weighted_st':\n",
    "                rmse, mae, xgauc, xauc = cal_reg_metric(self.vali_dat, self.model, self.vali_loader, self.all_dat, self.weight_name, self.c_model)\n",
    "            else:\n",
    "                rmse, mae, xgauc, xauc = 0, 0, 0, 0\n",
    "            self.early_stopping(mae, self.model, self.c_model)\n",
    "\n",
    "            if self.early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break \n",
    "\n",
    "            if epoch >= 0:\n",
    "                dur.append(time.time() - t0)\n",
    "\n",
    "            print(\"Epoch {:05d} | Time(s) {:.4f} | Train_Loss {:.4f} | Train_c_Loss {:.4f} | \"\n",
    "                        \"Vali_NDCG@1 {:.4f}| Vali_RMSE {:.4f}| Vali_MAE {:.4f}| Vali_GXAUC {:.4f}| Vali_XAUC {:.4f}|\". format(epoch, np.mean(dur), np.mean(loss_log),np.mean(c_loss_log),\n",
    "                                                                        0, rmse, mae, xgauc, xauc))\n",
    "    \n",
    "    # _test_and_save (使用 \"快速\" dcn.py 的逻辑)\n",
    "    def _test_and_save(self):\n",
    "        print(\"Testing...\")\n",
    "        text_embeddings_bundle = {\n",
    "            \"video_emb_tensor_cpu\": self.photo_embeddings,\n",
    "            \"video_id2idx\": self.video_id2idx,\n",
    "            \"comment_emb_tensor_cpu\": self.comment_embeddings,\n",
    "            \"comment_id2idx\": self.comment_id2idx\n",
    "        }\n",
    "        model = My_DeepCrossNetworkModel_withCommentsRanking(field_dims=cal_field_dims(self.all_dat, self.dat_name),\n",
    "                                                            comments_dims=cal_comments_dims(self.all_dat, self.dat_name),\n",
    "                                                            embed_dim=10, num_layers=3, mlp_dims=[64,64,64], dropout=0.2,\n",
    "                                                            text_embeddings=text_embeddings_bundle)\n",
    "        c_model = My_DeepCrossNetworkModel_withCommentsRanking(field_dims=cal_field_dims(self.all_dat, self.dat_name),\n",
    "                                                            comments_dims=cal_comments_dims(self.all_dat, self.dat_name),\n",
    "                                                            embed_dim=10, num_layers=3, mlp_dims=[64,64,64], dropout=0.2,\n",
    "                                                            text_embeddings=text_embeddings_bundle)\n",
    "\n",
    "        model = model.cuda()\n",
    "        c_model = c_model.cuda()\n",
    "\n",
    "        model.load_state_dict(torch.load(self.fout + '_temp_checkpoint.pt'))\n",
    "        c_model.load_state_dict(torch.load(self.fout + '_temp_usr_checkpoint.pt'))\n",
    "\n",
    "        ndcg_ls, pcr_ls, wt_ls, gauc_val, mrr_val= cal_group_metric(self.test_dat, c_model,[1,3,5], self.test_loader, dat_name=self.dat_name)\n",
    "\n",
    "        if self.weight_name == 'weighted_st':\n",
    "            rmse, mae, xgauc, xauc = cal_reg_metric(self.test_dat, model, self.test_loader, self.all_dat, self.weight_name, c_model)\n",
    "        else:\n",
    "            rmse, mae, xgauc, xauc = 0, 0, 0, 0\n",
    "\n",
    "        print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        print(\"{}_{} | Log_loss {:.4f} | AUC {:.4f} | GAUC {:.4f} | MRR {:.4f} | \"\n",
    "                    \"nDCG@1 {:.4f}| nDCG@3 {:.4f}| nDCG@5 {:.4f}| \"\n",
    "                    \"PCR@1 {:.4f}| PCR@3 {:.4f}| PCR@5 {:.4f}| WT@1 {:.4f}| WT@3 {:.4f}| WT@5 {:.4f}| RMSE {:.4f} | MAE {:.4f}| XGAUC {:.4f}| XAUC {:.4f}|\". format(self.model_name, self.label_name, 0,0, gauc_val, mrr_val,\n",
    "                                                                    ndcg_ls[0],ndcg_ls[1],ndcg_ls[2],pcr_ls[0],pcr_ls[1],pcr_ls[2],wt_ls[0],wt_ls[1],wt_ls[2], rmse, mae, xgauc, xauc))\n",
    "        print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        \n",
    "        df_result = pd.DataFrame([],columns=['GAUC','MRR','nDCG@1','nDCG@3','nDCG@5','PCR@1','PCR@3','PCR@5','WT@1','WT@3','WT@5','RMSE', 'MAE','XGAUC', 'XAUC'])\n",
    "        df_result.loc[1] =  [gauc_val, mrr_val] + ndcg_ls + pcr_ls + wt_ls + [rmse, mae, xgauc, xauc]\n",
    "\n",
    "        # (修正!) 保存结果时, 附加上 EMBEDDING_MODE\n",
    "        result_filename = f'{self.fout}_result_{self.EMBEDDING_MODE}.csv'\n",
    "        model_filename = f'{self.fout}_model_{self.EMBEDDING_MODE}.pt'\n",
    "        \n",
    "        df_result.to_csv(result_filename)\n",
    "        torch.save(model.state_dict(), model_filename)\n",
    "        print(f\"Results saved to: {result_filename}\")\n",
    "        print(f\"Model saved to: {model_filename}\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
